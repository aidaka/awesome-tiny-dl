# 每日思考和问题
## 2017.12.05
------
- dropout层为什么丢掉一定比例的神经元再训练效果就会好很多?有什么理论依据吗？
- 它是极端情况下的Bagging，由于在每步训练中，神经元会以某种概率随机被置为无效，相当于是参数共享的新网络结构，每个模型为了使损失降低会尽可能学最“本质”的特征，“本质”可以理解为由更加独立的、和其他神经元相关性弱的、泛化能力强的神经元提取出来的特征；而如果采用类似SGD的方式训练，每步迭代都会选取不同的数据集，这样整个网络相当于是用不同数据集学习的多个模型的集成组合。
- Relu的效果一定比sigmoid好吗？什么情况下更适合用sigmoid？
- 首先Relu可以避免在深层网络里使用Sigmoid出现的梯度消失问题

------
